{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d987ed55-cdfb-445d-b001-1bcb3d5dc5c5",
   "metadata": {},
   "source": [
    "### Preprocessing Module Synopsis\n",
    "\n",
    "In this preprocessing module, we clean, tokenize, and structure the raw conversational data from the **Cornell Movie Dialogs Corpus** (or any other selected dataset). This step is critical for transforming unstructured dialogue into a format suitable for training a generative-based chatbot. The key preprocessing tasks include removing unnecessary metadata, normalizing the text (lowercasing and removing special characters), tokenizing sentences, pairing input-response dialogues, and padding sequences to ensure consistency across all inputs.\n",
    "\n",
    "By completing this preprocessing, we prepare the data for the next phase: **model design and training**, where the chatbot will learn from these structured conversations. Proper preprocessing is crucial for ensuring the chatbot can generate coherent, context-aware responses during real-time conversations.\n",
    "\n",
    "Next steps include selecting an appropriate model architecture (e.g., Transformer, GPT) and training the chatbot using the preprocessed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62db3ac-b643-482f-ba0e-445b14ef323d",
   "metadata": {},
   "source": [
    "### Preprocessing steps:\n",
    "\n",
    "1. **Data Understanding**: Explore the structure and content of the dataset.\n",
    "2. **Data Cleaning**: \n",
    "   - Remove unnecessary metadata.\n",
    "   - Lowercase text.\n",
    "   - Remove special characters and punctuation.\n",
    "   - Remove empty or incomplete dialogues.\n",
    "3. **Tokenization**: Break down text into tokens (words or subwords).\n",
    "4. **Conversation Pairing**: Create (input, response) pairs for training.\n",
    "5. **Context Management** (optional): Group multiple turns of conversation.\n",
    "6. **Padding and Truncation**: Ensure all sequences are of fixed length.\n",
    "7. **Train/Test Split**: Divide the dataset into training and validation sets.\n",
    "8. **Special Token Handling**: Add special tokens like `<PAD>`, `<START>`, and `<END>`.\n",
    "9. **Vectorization/Encoding**: Convert tokens to numerical embeddings.\n",
    "10. **Save Preprocessed Data**: Store the cleaned and preprocessed data in a suitable format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8954473f-3457-4706-8ea4-7064b91c64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: convokit in ./.venv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in ./.venv/lib/python3.12/site-packages (from convokit) (3.9.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in ./.venv/lib/python3.12/site-packages (from convokit) (2.2.3)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in ./.venv/lib/python3.12/site-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=2.3.5 in ./.venv/lib/python3.12/site-packages (from convokit) (3.8.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./.venv/lib/python3.12/site-packages (from convokit) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./.venv/lib/python3.12/site-packages (from convokit) (1.5.2)\n",
      "Requirement already satisfied: nltk>=3.4 in ./.venv/lib/python3.12/site-packages (from convokit) (3.9.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in ./.venv/lib/python3.12/site-packages (from convokit) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.13.2 in ./.venv/lib/python3.12/site-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in ./.venv/lib/python3.12/site-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in ./.venv/lib/python3.12/site-packages (from convokit) (1.3.8)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./.venv/lib/python3.12/site-packages (from convokit) (4.66.5)\n",
      "Requirement already satisfied: pymongo>=4.0 in ./.venv/lib/python3.12/site-packages (from convokit) (4.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in ./.venv/lib/python3.12/site-packages (from convokit) (6.0.2)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in ./.venv/lib/python3.12/site-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from clean-text>=0.6.0->convokit) (6.2.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.0.0->convokit) (2.9.0.post0)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in ./.venv/lib/python3.12/site-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=0.23.4->convokit) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn>=0.20.0->convokit) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.12/site-packages (from spacy>=2.3.5->convokit) (3.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in ./.venv/lib/python3.12/site-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy>=2.3.5->convokit) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy>=2.3.5->convokit) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.18.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n",
      "Downloading movie-corpus to /Users/dimitridumont/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
      "Dataset already exists at /Users/dimitridumont/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "!pip install convokit transformers\n",
    "\n",
    "from convokit import Corpus, download\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from convokit import Corpus, download\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the Cornell Movie Dialogs Corpus\n",
    "corpus = Corpus(download(\"movie-corpus\"))\n",
    "corpus = Corpus(download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f572287f-ed8d-4683-a346-453a96b512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<convokit.model.corpus.Corpus object at 0x169abcf50>\n",
      "Conversation('id': 'cue8y0b', 'utterances': ['cue8y0b', 'cuec5fs', 'cuect48', 'cuedf8c', 'cuedywn', 'czb942p', 'czbbocu', 'czbdh6q', 'czbe470', 'czbe8el'], 'meta': ConvoKitMeta({'pair_id': 'cue8uxd', 'has_removed_comment': True, 'split': 'train', 'summary_meta': []}))\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Understanding\n",
    "# Check basic information about the corpus\n",
    "print(corpus)\n",
    "\n",
    "# Access a sample conversation and utterance\n",
    "for convo in corpus.iter_conversations():\n",
    "    print(convo)  # Print one conversation as an example\n",
    "    break  # Only show the first conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7069917a-2a51-4e07-9418-99f45b448f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 2. Data Cleaning\n",
    "#  cleaning function to lowercase and remove special characters and urls\n",
    "def clean_text(text):\n",
    "    # Dimitri - Remove URLs starting with http/https \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Match URLs starting with 'http' until a space\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean the utterances\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.text = clean_text(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac94ce51-a3a4-43bb-ae68-a849d777ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization using Hugging Face's GPT-2 tokenizer for consistency with GPT-2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.tokens = tokenize(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee80a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dimitridumont/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0ada468-3dbc-4530-a1a8-fdf4c5d5e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs after augmentation: 72244\n",
      "('okay ive seen this view come up a few times before and ive always been unsuccessful in convincing people about why theyre wrong however it seems that your view is based on studies so maybe youll respond well to evidence ucarlosriccy i cannot dispute the fact that there is a measurable iq gap bw white students and black students in the us however it is most certainly not because of genetics almost all of it can be attributed to social causes black students are often in schools that arent adequately funded poverty and its corollary effects etc the genetic basis for difference in iq has been tested you can find a comprehensive review here if you look at the science its clear that there is no evidence that justifies making inferences about race and intelligence but what about all those studies in the past that showed the evidence well its been shown that those studies mostly from the social sciences have been biased and unreliable you can read about it here im curious to see how you respond to evidence edit proceed with caution members or former members of rcoontown are here with their copypasta arguments', 'its not just black and white america though subsaharan africans and caribbeans of african decent by far score the lowest on average of ethnic groups with american blacks only performing marginally better and the tests dont measure anything that is taught they measure abstract reasoning thats why you can give an iq test to a child and also why gifted children generally can score higher when theyre young but usually see even if they are truly gifted somewhat of a drop as they grow older the test measures ones ability to conduct abstract reasoning against the ability of the mean a gifted child has advanced reasoning skills for their age but the gap between them and a normal child becomes less pronounced as the normal child catches up when they get older the cultural or class argument does not hold up because the test is designed to see how well you can detect patterns and relationships without any outside knowledge of a problem a pattern is a pattern is pattern for everyone thinking im some neonazi type playing white power whites also dont score the best asians do which also kills the idea of the concept is bias because white eurpeans invented it because if it was so culturally bias why would nonwhiteeuropeans perform better at it if you want to say abstract reasoning is an incomplete picture of intelligence and there are other factors and may components to intelligence you could say that and i probably wouldnt disagree with you but it has proven time and again that blacks perform the worst at abstract reasoning it makes people uncomfortable to talk about so people either dont talk about it or blame outside factors but we have over a century of testing at this point')\n"
     ]
    }
   ],
   "source": [
    "# 4. Data Augmentation - Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            # Randomly pick a synonym\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacing with the same word\n",
    "                new_words[i] = synonym\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# Apply data augmentation to increase dataset diversity\n",
    "augmented_pairs = []\n",
    "for convo in corpus.iter_conversations():\n",
    "    utterances = list(convo.iter_utterances())\n",
    "    for i in range(len(utterances) - 1):\n",
    "        input_text = utterances[i].text\n",
    "        response_text = utterances[i + 1].text\n",
    "        \n",
    "        # Apply synonym replacement\n",
    "        augmented_input = synonym_replacement(input_text)\n",
    "        augmented_response = synonym_replacement(response_text)\n",
    "        \n",
    "        # Original pair\n",
    "        augmented_pairs.append((input_text, response_text))\n",
    "        # Augmented pair\n",
    "        augmented_pairs.append((augmented_input, augmented_response))\n",
    "\n",
    "print(f\"Number of pairs after augmentation: {len(augmented_pairs)}\")\n",
    "print(augmented_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c6a9827-a3f9-452e-9b92-bb925d1e49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([482, 323, 220, 425, 1775, 428, 1570, 1282, 510, 257, 1178, 1661, 878, 290, 220, 425, 1464, 587, 23993, 287, 17101, 661, 546, 1521, 484, 260, 2642, 2158, 340, 2331, 326, 534, 1570, 318, 1912, 319, 3640, 523, 3863, 345, 297, 3031, 880, 284, 2370, 334, 66, 7063, 418, 1173, 948, 1312, 2314, 11047, 262, 1109, 326, 612, 318, 257, 40757, 1312, 80, 7625, 275, 86, 2330, 2444, 290, 2042, 2444, 287, 262, 514, 2158, 340, 318, 749, 3729, 407, 780, 286, 25862, 2048, 477, 286, 340, 460, 307, 14183, 284, 1919, 5640, 2042, 2444, 389, 1690, 287, 4266, 326], [896, 407, 655, 2042, 290, 2330, 45630, 64, 996, 6352, 993, 19173, 6580, 1173, 504, 290, 1097, 571, 44749, 286, 6580, 37189, 7709, 416, 1290, 4776, 262, 9016, 319, 2811, 286, 9450, 2628, 351, 45630, 272, 15102, 691, 9489, 44108, 1365, 290, 262, 5254, 17666, 3953, 1997, 326, 318, 7817, 484, 3953, 12531, 14607, 29294, 1521, 345, 460, 1577, 281, 1312, 80, 1332, 284, 257, 1200, 290, 635, 1521, 22527, 1751, 4143, 460, 4776, 2440, 618, 484, 260, 1862, 475, 3221, 766, 772, 611, 484, 389, 4988, 22527, 6454, 286, 257, 4268, 355, 484, 1663, 4697, 262, 1332, 5260, 3392])\n"
     ]
    }
   ],
   "source": [
    "# 5. Padding and Truncation using advanced tokenization\n",
    "\n",
    "\n",
    "# If the truncation cuts off important parts of the conversation\n",
    "# we might want to increase the max_length to allow longer sequences, especially for longer dialogues.\n",
    "max_length = 60 # Dimitri - Increased to 60\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, max_length):\n",
    "    padded_sequence = sequence[:max_length]\n",
    "    if len(sequence) < max_length:\n",
    "        padded_sequence += [0] * (max_length - len(sequence))\n",
    "    return padded_sequence\n",
    "\n",
    "# Tokenize, pad, and truncate the sequences\n",
    "for i, (input_text, response_text) in enumerate(augmented_pairs):\n",
    "    input_tokens = tokenizer.encode(input_text)\n",
    "    response_tokens = tokenizer.encode(response_text)\n",
    "    augmented_pairs[i] = (pad_sequence(input_tokens, max_length), pad_sequence(response_tokens, max_length))\n",
    "\n",
    "# Sample padded pair\n",
    "print(augmented_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "164f7808-b8e7-45f2-a556-8974c35905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 57795, Validation set size: 14449\n"
     ]
    }
   ],
   "source": [
    "# 6. rain/Test Split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.2)\n",
    "\n",
    "# Print sizes of train and validation sets\n",
    "print(f\"Train set size: {len(train_pairs)}, Validation set size: {len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23cd4204-3287-4fb4-8bec-cc49980f38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 7. Save Preprocessed Data\n",
    "with open('preprocessed_data.json', 'w') as f:\n",
    "    json.dump(augmented_pairs, f)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34cd96fa-9497-4416-b8fa-97dad24fc26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[482, 323, 220, 425, 1775, 428, 1570, 1282, 510, 257, 1178, 1661, 878, 290, 220, 425, 1464, 587, 23993, 287, 17101, 661, 546, 1521, 484, 260, 2642, 2158, 340, 2331, 326, 534, 1570, 318, 1912, 319, 3640, 523, 3863, 345, 297, 3031, 880, 284, 2370, 334, 66, 7063, 418, 1173, 948, 1312, 2314, 11047, 262, 1109, 326, 612, 318, 257, 40757, 1312, 80, 7625, 275, 86, 2330, 2444, 290, 2042, 2444, 287, 262, 514, 2158, 340, 318, 749, 3729, 407, 780, 286, 25862, 2048, 477, 286, 340, 460, 307, 14183, 284, 1919, 5640, 2042, 2444, 389, 1690, 287, 4266, 326], [896, 407, 655, 2042, 290, 2330, 45630, 64, 996, 6352, 993, 19173, 6580, 1173, 504, 290, 1097, 571, 44749, 286, 6580, 37189, 7709, 416, 1290, 4776, 262, 9016, 319, 2811, 286, 9450, 2628, 351, 45630, 272, 15102, 691, 9489, 44108, 1365, 290, 262, 5254, 17666, 3953, 1997, 326, 318, 7817, 484, 3953, 12531, 14607, 29294, 1521, 345, 460, 1577, 281, 1312, 80, 1332, 284, 257, 1200, 290, 635, 1521, 22527, 1751, 4143, 460, 4776, 2440, 618, 484, 260, 1862, 475, 3221, 766, 772, 611, 484, 389, 4988, 22527, 6454, 286, 257, 4268, 355, 484, 1663, 4697, 262, 1332, 5260, 3392]]\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "with open('preprocessed_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print a sample of the data to check\n",
    "print(data[0])  # Should display a tuple of tokenized, padded input and response sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a18eb5-8c38-40a0-9b6f-efc42771e797",
   "metadata": {},
   "source": [
    "#### Preprocessed Data Explanation / Usage :\n",
    "\n",
    "1. **Tokenized Sequences**:\n",
    "   - Each list inside the outer list represents a sequence of tokenized words (input and response) from the preprocessed data. The numbers are token IDs, which are the result of passing the text through the tokenizer (in this case, GPT-2's tokenizer).\n",
    "\n",
    "2. **Padded Sequences**:\n",
    "   - The sequences have been padded (or truncated) to a fixed length (`max_length = 20` in our case). The list of numbers should represent tokenized text that was either truncated or padded as part of the preprocessing step.\n",
    "\n",
    "### Explanation of Output:\n",
    "- `data[0]` is a tuple of two lists (input and response).\n",
    "   - The first list `[482, 323, 220, ...]` is the tokenized and padded sequence for the input sentence.\n",
    "   - The second list `[896, 407, 655, ...]` is the tokenized and padded sequence for the response sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "160deb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[482, 323, 220, 425, 1775, 428, 1570, 1282, 510, 257, 1178, 1661, 878, 290, 220, 425, 1464, 587, 23993, 287, 17101, 661, 546, 1521, 484, 260, 2642, 2158, 340, 2331, 326, 534, 1570, 318, 1912, 319, 3640, 523, 3863, 345, 297, 3031, 880, 284, 2370, 334, 66, 7063, 418, 1173, 948, 1312, 2314, 11047, 262, 1109, 326, 612, 318, 257, 40757, 1312, 80, 7625, 275, 86, 2330, 2444, 290, 2042, 2444, 287, 262, 514, 2158, 340, 318, 749, 3729, 407, 780, 286, 25862, 2048, 477, 286, 340, 460, 307, 14183, 284, 1919, 5640, 2042, 2444, 389, 1690, 287, 4266, 326]\n",
      "[896, 407, 655, 2042, 290, 2330, 45630, 64, 996, 6352, 993, 19173, 6580, 1173, 504, 290, 1097, 571, 44749, 286, 6580, 37189, 7709, 416, 1290, 4776, 262, 9016, 319, 2811, 286, 9450, 2628, 351, 45630, 272, 15102, 691, 9489, 44108, 1365, 290, 262, 5254, 17666, 3953, 1997, 326, 318, 7817, 484, 3953, 12531, 14607, 29294, 1521, 345, 460, 1577, 281, 1312, 80, 1332, 284, 257, 1200, 290, 635, 1521, 22527, 1751, 4143, 460, 4776, 2440, 618, 484, 260, 1862, 475, 3221, 766, 772, 611, 484, 389, 4988, 22527, 6454, 286, 257, 4268, 355, 484, 1663, 4697, 262, 1332, 5260, 3392]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0])  # Check if this is a list of integers\n",
    "print(data[0][1])  # Check if this is a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afaa48a9-104d-4713-befe-3b728f33984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: okay ive seen this view come up a few times before and ive always been unsuccessful in convincing people about why theyre wrong however it seems that your view is based on studies so maybe youll respond well to evidence ucarlosriccy i cannot dispute the fact that there is a measurable iq gap bw white students and black students in the us however it is most certainly not because of genetics almost all of it can be attributed to social causes black students are often in schools that\n",
      "Response text: its not just black and white america though subsaharan africans and caribbeans of african decent by far score the lowest on average of ethnic groups with american blacks only performing marginally better and the tests dont measure anything that is taught they measure abstract reasoning thats why you can give an iq test to a child and also why gifted children generally can score higher when theyre young but usually see even if they are truly gifted somewhat of a drop as they grow older the test measures ones\n"
     ]
    }
   ],
   "source": [
    "# ^ These token IDs can be mapped back to their original words using the tokenizer if you want to check the original text:\n",
    "\n",
    "# Convert token IDs back to words to verify the text\n",
    "input_text = tokenizer.decode(data[0][0], skip_special_tokens=True)\n",
    "response_text = tokenizer.decode(data[0][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Response text:\", response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
