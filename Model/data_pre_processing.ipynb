{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d987ed55-cdfb-445d-b001-1bcb3d5dc5c5",
   "metadata": {},
   "source": [
    "### Preprocessing Module Synopsis\n",
    "\n",
    "In this preprocessing module, we clean, tokenize, and structure the raw conversational data from the **Cornell Movie Dialogs Corpus** (or any other selected dataset). This step is critical for transforming unstructured dialogue into a format suitable for training a generative-based chatbot. The key preprocessing tasks include removing unnecessary metadata, normalizing the text (lowercasing and removing special characters), tokenizing sentences, pairing input-response dialogues, and padding sequences to ensure consistency across all inputs.\n",
    "\n",
    "By completing this preprocessing, we prepare the data for the next phase: **model design and training**, where the chatbot will learn from these structured conversations. Proper preprocessing is crucial for ensuring the chatbot can generate coherent, context-aware responses during real-time conversations.\n",
    "\n",
    "Next steps include selecting an appropriate model architecture (e.g., Transformer, GPT) and training the chatbot using the preprocessed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62db3ac-b643-482f-ba0e-445b14ef323d",
   "metadata": {},
   "source": [
    "### Preprocessing steps:\n",
    "\n",
    "1. **Data Understanding**: Explore the structure and content of the dataset.\n",
    "2. **Data Cleaning**: \n",
    "   - Remove unnecessary metadata.\n",
    "   - Lowercase text.\n",
    "   - Remove special characters and punctuation.\n",
    "   - Remove empty or incomplete dialogues.\n",
    "3. **Tokenization**: Break down text into tokens (words or subwords).\n",
    "4. **Conversation Pairing**: Create (input, response) pairs for training.\n",
    "5. **Context Management** (optional): Group multiple turns of conversation.\n",
    "6. **Padding and Truncation**: Ensure all sequences are of fixed length.\n",
    "7. **Train/Test Split**: Divide the dataset into training and validation sets.\n",
    "8. **Special Token Handling**: Add special tokens like `<PAD>`, `<START>`, and `<END>`.\n",
    "9. **Vectorization/Encoding**: Convert tokens to numerical embeddings.\n",
    "10. **Save Preprocessed Data**: Store the cleaned and preprocessed data in a suitable format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8954473f-3457-4706-8ea4-7064b91c64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: convokit in /usr/local/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/site-packages (from convokit) (3.7.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/site-packages (from convokit) (2.1.0)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.11/site-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.11/site-packages (from convokit) (3.7.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/site-packages (from convokit) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/site-packages (from convokit) (1.5.0)\n",
      "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.11/site-packages (from convokit) (3.8.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.11/site-packages (from convokit) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/site-packages (from convokit) (1.3.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.11/site-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.11/site-packages (from convokit) (1.3.8)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/site-packages (from convokit) (4.66.1)\n",
      "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.11/site-packages (from convokit) (4.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/site-packages (from convokit) (6.0.1)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.11/site-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.11/site-packages (from clean-text>=0.6.0->convokit) (6.2.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.11/site-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.4->convokit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.4->convokit) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (69.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.11/site-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/obosieakioyamen/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "!pip install convokit transformers\n",
    "\n",
    "from convokit import Corpus, download\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from convokit import Corpus, download\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the Cornell Movie Dialogs Corpus\n",
    "corpus = Corpus(download(\"movie-corpus\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f572287f-ed8d-4683-a346-453a96b512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<convokit.model.corpus.Corpus object at 0x13bae8f10>\n",
      "Conversation('id': 'L1044', 'utterances': ['L1045', 'L1044'], 'meta': ConvoKitMeta({'movie_idx': 'm0', 'movie_name': '10 things i hate about you', 'release_year': '1999', 'rating': '6.90', 'votes': '62847', 'genre': \"['comedy', 'romance']\"}))\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Understanding\n",
    "# Check basic information about the corpus\n",
    "print(corpus)\n",
    "\n",
    "# Access a sample conversation and utterance\n",
    "for convo in corpus.iter_conversations():\n",
    "    print(convo)  # Print one conversation as an example\n",
    "    break  # Only show the first conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7069917a-2a51-4e07-9418-99f45b448f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 2. Data Cleaning\n",
    "#  cleaning function to lowercase and remove special characters and urls\n",
    "def clean_text(text):\n",
    "    # Dimitri - Remove URLs starting with http/https \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Match URLs starting with 'http' until a space\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean the utterances\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.text = clean_text(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac94ce51-a3a4-43bb-ae68-a849d777ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization using Hugging Face's GPT-2 tokenizer for consistency with GPT-2\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.tokens = tokenize(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee80a85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/obosieakioyamen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ada468-3dbc-4530-a1a8-fdf4c5d5e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs after augmentation: 443232\n",
      "('they do not', 'they do to')\n"
     ]
    }
   ],
   "source": [
    "# 4. Data Augmentation - Synonym Replacement\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for i, word in enumerate(words):\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            # Randomly pick a synonym\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != word:  # Avoid replacing with the same word\n",
    "                new_words[i] = synonym\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# Apply data augmentation to increase dataset diversity\n",
    "augmented_pairs = []\n",
    "for convo in corpus.iter_conversations():\n",
    "    utterances = list(convo.iter_utterances())\n",
    "    for i in range(len(utterances) - 1):\n",
    "        input_text = utterances[i].text\n",
    "        response_text = utterances[i + 1].text\n",
    "        \n",
    "        # Apply synonym replacement\n",
    "        augmented_input = synonym_replacement(input_text)\n",
    "        augmented_response = synonym_replacement(response_text)\n",
    "        \n",
    "        # Original pair\n",
    "        augmented_pairs.append((input_text, response_text))\n",
    "        # Augmented pair\n",
    "        augmented_pairs.append((augmented_input, augmented_response))\n",
    "\n",
    "print(f\"Number of pairs after augmentation: {len(augmented_pairs)}\")\n",
    "print(augmented_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6a9827-a3f9-452e-9b92-bb925d1e49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9930, 466, 407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [9930, 466, 284, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 5. Padding and Truncation using advanced tokenization\n",
    "\n",
    "\n",
    "# If the truncation cuts off important parts of the conversation\n",
    "# we might want to increase the max_length to allow longer sequences, especially for longer dialogues.\n",
    "max_length = 60 # Dimitri - Increased to 60\n",
    "\n",
    "\n",
    "def pad_sequence(sequence, max_length):\n",
    "    padded_sequence = sequence[:max_length]\n",
    "    if len(sequence) < max_length:\n",
    "        padded_sequence += [0] * (max_length - len(sequence))\n",
    "    return padded_sequence\n",
    "\n",
    "# Tokenize, pad, and truncate the sequences\n",
    "for i, (input_text, response_text) in enumerate(augmented_pairs):\n",
    "    input_tokens = tokenizer.encode(input_text)\n",
    "    response_tokens = tokenizer.encode(response_text)\n",
    "    augmented_pairs[i] = (pad_sequence(input_tokens, max_length), pad_sequence(response_tokens, max_length))\n",
    "\n",
    "# Sample padded pair\n",
    "print(augmented_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164f7808-b8e7-45f2-a556-8974c35905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 354585, Validation set size: 88647\n"
     ]
    }
   ],
   "source": [
    "# 6. rain/Test Split\n",
    "train_pairs, val_pairs = train_test_split(augmented_pairs, test_size=0.2)\n",
    "\n",
    "# Print sizes of train and validation sets\n",
    "print(f\"Train set size: {len(train_pairs)}, Validation set size: {len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cd4204-3287-4fb4-8bec-cc49980f38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 7. Save Preprocessed Data\n",
    "with open('preprocessed_data.json', 'w') as f:\n",
    "    json.dump(augmented_pairs, f)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34cd96fa-9497-4416-b8fa-97dad24fc26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9930, 466, 407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [9930, 466, 284, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "with open('preprocessed_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print a sample of the data to check\n",
    "print(data[0])  # Should display a tuple of tokenized, padded input and response sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a18eb5-8c38-40a0-9b6f-efc42771e797",
   "metadata": {},
   "source": [
    "#### Preprocessed Data Explanation / Usage :\n",
    "\n",
    "1. **Tokenized Sequences**:\n",
    "   - Each list inside the outer list represents a sequence of tokenized words (input and response) from the preprocessed data. The numbers are token IDs, which are the result of passing the text through the tokenizer (in this case, GPT-2's tokenizer).\n",
    "\n",
    "2. **Padded Sequences**:\n",
    "   - The sequences have been padded (or truncated) to a fixed length (`max_length = 20` in our case). The list of numbers should represent tokenized text that was either truncated or padded as part of the preprocessing step.\n",
    "\n",
    "### Explanation of Output:\n",
    "- `data[0]` is a tuple of two lists (input and response).\n",
    "   - The first list `[482, 323, 220, ...]` is the tokenized and padded sequence for the input sentence.\n",
    "   - The second list `[896, 407, 655, ...]` is the tokenized and padded sequence for the response sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160deb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9930, 466, 407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[9930, 466, 284, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0])  # Check if this is a list of integers\n",
    "print(data[0][1])  # Check if this is a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afaa48a9-104d-4713-befe-3b728f33984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 15:22:49.736780: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: they do not!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Response text: they do to!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# ^ These token IDs can be mapped back to their original words using the tokenizer if you want to check the original text:\n",
    "\n",
    "# Convert token IDs back to words to verify the text\n",
    "input_text = tokenizer.decode(data[0][0], skip_special_tokens=True)\n",
    "response_text = tokenizer.decode(data[0][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Response text:\", response_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
