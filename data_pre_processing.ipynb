{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d987ed55-cdfb-445d-b001-1bcb3d5dc5c5",
   "metadata": {},
   "source": [
    "### Preprocessing Module Synopsis\n",
    "\n",
    "In this preprocessing module, we clean, tokenize, and structure the raw conversational data from the **Cornell Movie Dialogs Corpus** (or any other selected dataset). This step is critical for transforming unstructured dialogue into a format suitable for training a generative-based chatbot. The key preprocessing tasks include removing unnecessary metadata, normalizing the text (lowercasing and removing special characters), tokenizing sentences, pairing input-response dialogues, and padding sequences to ensure consistency across all inputs.\n",
    "\n",
    "By completing this preprocessing, we prepare the data for the next phase: **model design and training**, where the chatbot will learn from these structured conversations. Proper preprocessing is crucial for ensuring the chatbot can generate coherent, context-aware responses during real-time conversations.\n",
    "\n",
    "Next steps include selecting an appropriate model architecture (e.g., Transformer, GPT) and training the chatbot using the preprocessed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62db3ac-b643-482f-ba0e-445b14ef323d",
   "metadata": {},
   "source": [
    "### Preprocessing steps:\n",
    "\n",
    "1. **Data Understanding**: Explore the structure and content of the dataset.\n",
    "2. **Data Cleaning**: \n",
    "   - Remove unnecessary metadata.\n",
    "   - Lowercase text.\n",
    "   - Remove special characters and punctuation.\n",
    "   - Remove empty or incomplete dialogues.\n",
    "3. **Tokenization**: Break down text into tokens (words or subwords).\n",
    "4. **Conversation Pairing**: Create (input, response) pairs for training.\n",
    "5. **Context Management** (optional): Group multiple turns of conversation.\n",
    "6. **Padding and Truncation**: Ensure all sequences are of fixed length.\n",
    "7. **Train/Test Split**: Divide the dataset into training and validation sets.\n",
    "8. **Special Token Handling**: Add special tokens like `<PAD>`, `<START>`, and `<END>`.\n",
    "9. **Vectorization/Encoding**: Convert tokens to numerical embeddings.\n",
    "10. **Save Preprocessed Data**: Store the cleaned and preprocessed data in a suitable format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8954473f-3457-4706-8ea4-7064b91c64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: convokit in /usr/local/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/site-packages (from convokit) (3.7.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/site-packages (from convokit) (2.1.0)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.11/site-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.11/site-packages (from convokit) (3.7.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/site-packages (from convokit) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/site-packages (from convokit) (1.5.0)\n",
      "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.11/site-packages (from convokit) (3.8.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.11/site-packages (from convokit) (0.3.9)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/site-packages (from convokit) (1.3.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.11/site-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.11/site-packages (from convokit) (1.3.8)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/site-packages (from convokit) (4.66.1)\n",
      "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.11/site-packages (from convokit) (4.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/site-packages (from convokit) (6.0.1)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.11/site-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.11/site-packages (from clean-text>=0.6.0->convokit) (6.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.11/site-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk>=3.4->convokit) (2024.5.15)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.4->convokit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas>=0.23.4->convokit) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (0.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (69.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/site-packages (from spacy>=2.3.5->convokit) (3.4.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.11/site-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.3.5->convokit) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.3.5->convokit) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.3.5->convokit) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to /Users/obosieakioyamen/.convokit/downloads/movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
      "Dataset already exists at /Users/obosieakioyamen/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "!pip install convokit\n",
    "\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# Download the Cornell Movie Dialogs Corpus\n",
    "corpus = Corpus(download(\"movie-corpus\"))\n",
    "corpus = Corpus(download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f572287f-ed8d-4683-a346-453a96b512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<convokit.model.corpus.Corpus object at 0x13f9b3ed0>\n",
      "Conversation('id': 'cue8y0b', 'utterances': ['cue8y0b', 'cuec5fs', 'cuect48', 'cuedf8c', 'cuedywn', 'czb942p', 'czbbocu', 'czbdh6q', 'czbe470', 'czbe8el'], 'meta': ConvoKitMeta({'pair_id': 'cue8uxd', 'has_removed_comment': True, 'split': 'train', 'summary_meta': []}))\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Understanding\n",
    "# Check basic information about the corpus\n",
    "print(corpus)\n",
    "\n",
    "# Access a sample conversation and utterance\n",
    "for convo in corpus.iter_conversations():\n",
    "    print(convo)  # Print one conversation as an example\n",
    "    break  # Only show the first conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7069917a-2a51-4e07-9418-99f45b448f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 2. Data Cleaning\n",
    "#  cleaning function to lowercase and remove special characters\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Clean the utterances\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.text = clean_text(utt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac94ce51-a3a4-43bb-ae68-a849d777ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenization\n",
    "# tokenization using basic Python split we can also use libraries like NLTK/spaCy \n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    for utt in convo.iter_utterances():\n",
    "        utt.tokens = tokenize(utt.text)  # Tokenize the utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead3c934-b2d1-4b15-9dbd-16d5521ab0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('okay ive seen this view come up a few times before and ive always been unsuccessful in convincing people about why theyre wrong however it seems that your view is based on studies so maybe youll respond well to evidence ucarlosriccy\\n\\ni cannot dispute the fact that there is a measurable iq gap bw white students and black students in the us however it is most certainly not because of genetics almost all of it can be attributed to social causes  black students are often in schools that arent adequately funded poverty and its corollary effects etc \\n\\nthe genetic basis for difference in iq has been tested you can find a comprehensive review herehttpwwwncbinlmnihgovpmcarticlespmc3341646 if you look at the science its clear that there is no evidence that justifies making inferences about race and intelligence\\n\\nbut what about all those studies in the past that showed the evidence well its been shown that those studies mostly from the social sciences have been biased and unreliable you can read about it herehttpwwwncbinlmnihgovpubmed15641921\\n\\nim curious to see how you respond to evidence\\n\\nedit proceed with caution members or former members of rcoontown are here with their copypasta arguments', 'its not just black and white america though subsaharan africans and caribbeans of african decent by far score the lowest on average of ethnic groups with american blacks only performing marginally better and the tests dont measure anything that is taught they measure abstract reasoning thats why you can give an iq test to a child and also why gifted children generally can score higher when theyre young but usually see even if they are truly gifted somewhat of a drop as they grow older the test measures ones ability to conduct abstract reasoning against the ability of the mean a gifted child has advanced reasoning skills for their age but the gap between them and a normal child becomes less pronounced as the normal child catches up when they get older the cultural or class argument does not hold up because the test is designed to see how well you can detect patterns and relationships without any outside knowledge of a problem a pattern is a pattern is pattern\\n\\nfor everyone thinking im some neonazi type playing white power whites also dont score the best  asians do which also kills the idea of the concept is bias because white eurpeans invented it because if it was so culturally bias why would nonwhiteeuropeans perform better at it\\n\\nif you want to say abstract reasoning is an incomplete picture of intelligence and there are other factors and may components to intelligence you could say that and i probably wouldnt disagree with you but it has proven time and again that blacks perform the worst at abstract reasoning it makes people uncomfortable to talk about so people either dont talk about it or blame outside factors but we have over a century of testing at this point ')\n"
     ]
    }
   ],
   "source": [
    "# 4. Conversation Pairing\n",
    "# Extract input-response pairs\n",
    "pairs = []\n",
    "for convo in corpus.iter_conversations():\n",
    "    utterances = list(convo.iter_utterances())\n",
    "    for i in range(len(utterances) - 1):\n",
    "        pairs.append((utterances[i].text, utterances[i+1].text))\n",
    "\n",
    "print(pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6a9827-a3f9-452e-9b92-bb925d1e49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['okay', 'ive', 'seen', 'this', 'view', 'come', 'up', 'a', 'few', 'times'], ['its', 'not', 'just', 'black', 'and', 'white', 'america', 'though', 'subsaharan', 'africans'])\n"
     ]
    }
   ],
   "source": [
    "# 5. Padding and Truncation\n",
    "max_length = 10\n",
    "def pad_sequence(sequence, max_length):\n",
    "    return sequence[:max_length] + ['<PAD>'] * (max_length - len(sequence))\n",
    "\n",
    "# Example padding, need to make better if model traning needs it \n",
    "for i, (input_text, response_text) in enumerate(pairs):\n",
    "    input_tokens = tokenize(input_text)\n",
    "    response_tokens = tokenize(response_text)\n",
    "    pairs[i] = (pad_sequence(input_tokens, max_length), pad_sequence(response_tokens, max_length))\n",
    "\n",
    "#sample padded pair\n",
    "print(pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164f7808-b8e7-45f2-a556-8974c35905a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 28897, Validation set size: 7225\n"
     ]
    }
   ],
   "source": [
    "# 6. Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.2)\n",
    "\n",
    "# Print sizes of train and validation sets\n",
    "print(f\"Train set size: {len(train_pairs)}, Validation set size: {len(val_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cd4204-3287-4fb4-8bec-cc49980f38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 7. Save Preprocessed Data\n",
    "import json\n",
    "\n",
    "]with open('preprocessed_data.json', 'w') as f:\n",
    "    json.dump(pairs, f)\n",
    "print(\"Preprocessed data saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
